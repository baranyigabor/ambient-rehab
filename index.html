<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Nerfies: Deformable Neural Radiance Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


  Gábor Baranyi, Zsolt Csibi, Kristian Fenech, Áron Fóthi, Zsófia Gaál, Joul Skaf, András Lőrincz
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Adaptive Assistance Framework for Ambient Intelligence
            Rehabilitation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Gábor Baranyi</a>,
            </span>
            <span class="author-block">
              <a href="">Zsolt Csibi</a>,
            </span>
            <span class="author-block">
              <a href="">Kristian Fenech</a>
            </span>
            <span class="author-block">
              <a href="">Áron Fóthi</a>,
            </span>
            <span class="author-block">
              <a href="">Zsófia Gaál</a>,
            </span>
            <span class="author-block">
              <a href="">Joul Skaf</a>,
            </span>
            <span class="author-block">
              <a href="">András Lőrincz</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Dept. of Artificial Intelligence, Eötvös Loránd University, Budapest, H-1117, Hungary.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/baranyigabor/ambient-rehab"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Supplementary Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1EiVm_1wZl7K9ygNam9lSOoFYbQz9f8A6"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-folder-open"></i>
                  </span>
                  <span>Supplementary</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="hero-body has-text-centered">
  <style>
    .resized-image {
        width: 55%; /* Resizes image to 50% of its original size */
        height: auto; /* Maintains aspect ratio */
    }
  </style>  
  <img src="./static/images/Pipeline.png" alt="pipeline" class="resized-image">
  <div class="container is-max-desktop has-text-justified">
    (a) Patient with smartphone, (b) 3D patient model reconstruction (<a href="https://arxiv.org/abs/2008.09062">Choutas and et al., 2020</a>),
(c) 3D NeRF or Gaussian Splat of the home environment (<a href="https://arxiv.org/abs/1904.01201">Savva and et al., 2019</a>) reconstruction with
NeRFStudio (<a href="https://arxiv.org/abs/2302.04264">Tancik and et al., 2023</a>), (d) colored semantic map for path planning to the exercise area, (e)
optimal position of mobile tripod and patient, (f) optimal pose of camera and (g) patient, (h) navigation
module guides the human partner to set up the tripod and to go to the start position and take the starting
pose, (i) Visual and verbal instructions for the exercise delivered via a mobile phone, video recording,
background elimination, conversion recording into a skeleton and an avatar representation for privacy
reasons, (j) videos are shown about the correct and the time warped execution of exercises for visual
comparison, (both (i) and (J) are shown in the clinical environment) (k) VLM verbal feedback to the
therapist provides information about potential errors. The design has resources from <a href="https://www.flaticon.com/">Flaticon.com</a>.
  </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
        <p>
          This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS) framework, an
advanced artificial intelligence-based solution tailored for home rehabilitation environments. AIRS
integrates cutting-edge technologies, including Real-Time 3D Reconstruction (RT-3DR), intelligent
navigation, and large Vision-Language Models (VLMs), to create a comprehensive system for machine-
guided physical rehabilitation. The framework is demonstrated in rehabilitation scenarios following
total knee replacement (TKR), utilizing a database of 263 video recordings for evaluation. A smart-
phone is employed within AIRS to perform RT-3DR of living spaces and has a body-matched avatar
to provide visual feedback about the excercise. This avatar is necessary in (a) optimizing exercise
configurations, including camera placement, patient positioning, and initial poses, and (b) address-
ing privacy concerns and promoting compliance with the AI Act. 
        </p>

<p>The system guides users through
the recording process to ensure the collection of properly recorded videos. AIRS employs two feed-
back mechanisms: (i) visual 3D feedback, enabling direct comparisons between prerecorded clinical
exercises and patient home recordings and (ii) VLM-generated feedback, providing detailed explana-
tions and corrections for exercise errors. The framework also supports people with visual and hearing
impairments. It also features a modular design that can be adapted to broader rehabilitation contexts.
AIRS software components are available for further use and customization. </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{,
  author    = {},
  title     = {},
  journal   = {},
  year      = {},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>The webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies.</a></p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
